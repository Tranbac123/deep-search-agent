# ğŸ” Deep Search Agent

Command-line research assistant that plans web queries, gathers evidence, and summarizes answers through an LLM-driven workflow. Works fully offline (stub retriever + local LLM) or online (DuckDuckGo + OpenAI).

## ğŸš§ Features at a Glance

- Multi-step workflow (plan â†’ search â†’ aggregate â†’ summarize)
- Pluggable LLM backend (`BaseLLM`) with OpenAI or local stub
- Swappable retrievers (`BaseRetriever`) with DuckDuckGo or stub data
- CLI with REPL and single-run modes, JSON output for automation
- Easy embedding via `DeepSearchAgent.from_settings(...)`

## âš™ï¸ Installation

```bash
git clone https://github.com/your-org/deep-search-agent
cd deep-search-agent
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -e .
cp .env.example .env  # optional; set OPENAI_API_KEY etc. if using online mode
```

### Environment options

- `DEEPSEARCH_OFFLINE=true` (or `--offline`) keeps everything local with stub data.
- Set `OPENAI_API_KEY=sk-...` to enable the OpenAI backend; otherwise the CLI quietly falls back to the local LLM.
- Other knobs (LLM model, top_k, etc.) live in `.env.example` and can be overridden via CLI flags.

## ğŸ–¥ï¸ CLI Usage

### Offline mode (default when `DEEPSEARCH_OFFLINE=true` or `--offline`)
```bash
python main.py --offline
```
This uses the local stub LLM + stub retriever (no network/API keys needed).

### Online mode (requires `OPENAI_API_KEY`)
```bash
export OPENAI_API_KEY=sk-...
python main.py --workflow production
```

### REPL mode (default)
```bash
python main.py --offline
ğŸ” Query (or 'exit'): best open-source LLMs
...
```
Add `--once` to exit after the first answer.

### Single query + JSON output
```bash
python main.py --offline --json "compare fastapi and flask"
```
Prints structured JSON with `query`, `plan`, `answer`, and `sources`.

### Useful flags
- `--offline` â€“ force local LLM + stub retriever
- `--workflow {basic,production,langgraph}` â€“ choose workflow
- `--llm-provider {openai,local}` â€“ override backend
- `--top-k N` â€“ limit retrieved documents per step
- `--json` â€“ emit JSON instead of prose
- positional `query` â€“ run once and exit
- `--once` â€“ exit after first REPL answer

## ğŸ§© Embedding as a Library

```python
from deep_search_agent.config import get_settings
from deep_search_agent.agents.deep_search_agent import DeepSearchAgent

settings = get_settings().with_overrides(offline=True)
agent = DeepSearchAgent.from_settings(settings, workflow_name="production")
result = agent.run("best vector databases 2024")

print(result.summary)
for source in result.findings:
    print(source.title, source.url)
```

`AgentResult` exposes `summary`, `plan`, and `findings`, plus `to_dict()` for serialization.

## ğŸ§ª Tests

All tests run offline using stubs:

```bash
pytest
```

## ğŸ“ Project Layout

```
deep-search-agent/
â”œâ”€â”€ main.py                     # CLI entrypoint
â”œâ”€â”€ src/deep_search_agent/
â”‚   â”œâ”€â”€ cli/                    # CLI wiring
â”‚   â”œâ”€â”€ agents/                 # DeepSearchAgent + steps
â”‚   â”œâ”€â”€ workflows/              # basic / production / langgraph
â”‚   â”œâ”€â”€ models/                 # BaseLLM, OpenAI backend, Local backend
â”‚   â”œâ”€â”€ retrieval/              # BaseRetriever, DuckDuckGo, stub
â”‚   â”œâ”€â”€ prompts/, context/, infra/, tools/, utils/
â””â”€â”€ tests/                      # Offline unit/perf tests
```

## ğŸ“ Notes

- Offline mode is recommended for development and CI.
- Online mode only activates when `OPENAI_API_KEY` is set and `--offline` is not used.
- Extend or replace retrievers/LLMs by implementing the `BaseRetriever` / `BaseLLM` protocols.

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src tests/

# Run specific test
pytest tests/unit/test_workflow.py -v
```

## âš™ï¸ Configuration

Key settings in `.env`:

```bash
# LLM Configuration
LLM_MODEL=gpt-4o
LLM_TEMPERATURE=0.1

# RAG Configuration
EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
RAG_TOP_K=6
VECTOR_SIMILARITY_THRESHOLD=0.35

# Workflow Configuration
MAX_TOOLS_PER_QUERY=5
MAX_CONTENT_LENGTH=2000

# Rate Limiting
REQUESTS_PER_MINUTE=60
ENABLE_RATE_LIMITING=true

# Caching
ENABLE_CACHING=true
CACHE_TTL_HOURS=24
```

## ğŸ“ Project Structure

```
DEEP-SEARCH-AGENT/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env.example
â”œâ”€â”€ main.py                 # entrypoint: python main.py -> calls CLI
â”œâ”€â”€ src/
â”‚   â””â”€â”€ deep_search_agent/  # main package
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py       # Settings (model_backend, keys, timeouts,â€¦)
â”‚       â”‚
â”‚       â”œâ”€â”€ cli/            # Interface layer: handles CLI / terminal I/O only
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ app.py      # run_cli(): loop to read input -> call agent
â”‚       â”‚
â”‚       â”œâ”€â”€ agents/         # Core agent logic (no I/O dependency)
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ deep_search_agent.py   # DeepSearchAgent (plan + run)
â”‚       â”‚   â””â”€â”€ steps/      # if you want to break down into agentic steps
â”‚       â”‚       â”œâ”€â”€ __init__.py
â”‚       â”‚       â”œâ”€â”€ plan.py           # decompose query -> plan
â”‚       â”‚       â”œâ”€â”€ search.py         # call retrieval
â”‚       â”‚       â”œâ”€â”€ aggregate.py      # merge / filter results
â”‚       â”‚       â””â”€â”€ summarize.py      # summarize, format output
â”‚       â”‚
â”‚       â”œâ”€â”€ workflows/      # different flow versions
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ basic.py              # simple workflow (demo)
â”‚       â”‚   â”œâ”€â”€ production.py         # more production-ready workflow
â”‚       â”‚   â””â”€â”€ langgraph_based.py    # if using LangGraph
â”‚       â”‚
â”‚       â”œâ”€â”€ models/         # LLM backend abstraction (local & API)
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ base.py              # BaseLLM, BaseEmbeddings,...
â”‚       â”‚   â”œâ”€â”€ openai_backend.py    # using OpenAI / other API providers
â”‚       â”‚   â””â”€â”€ local_backend.py     # using local models (Ollama, vLLM,â€¦)
â”‚       â”‚
â”‚       â”œâ”€â”€ retrieval/      # search & RAG layer
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ base.py             # Retriever interface
â”‚       â”‚   â”œâ”€â”€ web_search.py       # call search APIs (Tavily, Serper,â€¦)
â”‚       â”‚   â”œâ”€â”€ crawler.py          # Firecrawl / custom crawling
â”‚       â”‚   â””â”€â”€ rag.py              # retrieve + chunk + rerank (if needed)
â”‚       â”‚
â”‚       â”œâ”€â”€ prompts/        # prompt templates for each step
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ system.py           # global system prompt
â”‚       â”‚   â”œâ”€â”€ search_prompt.py
â”‚       â”‚   â””â”€â”€ summarize_prompt.py
â”‚       â”‚
â”‚       â”œâ”€â”€ context/        # context/memory management (if needed)
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ memory.py          # short-term / long-term memory
â”‚       â”‚
â”‚       â”œâ”€â”€ infra/          # shared infrastructure (cross-cutting)
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ cache.py           # cache web/LLM results
â”‚       â”‚   â”œâ”€â”€ rate_limiter.py
â”‚       â”‚   â””â”€â”€ logger.py
â”‚       â”‚
â”‚       â”œâ”€â”€ tools/          # wrappers as "Tools" for agent
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ firecrawl_client.py
â”‚       â”‚   â””â”€â”€ web_search_tool.py  # uses Retriever + format results for LLM
â”‚       â”‚
â”‚       â””â”€â”€ utils/          # small helpers, pure logic
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ text.py            # clean_text, truncate, highlight,...
â”‚           â””â”€â”€ timing.py          # measure time, simple metrics
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ unit/
    â”‚   â”œâ”€â”€ test_agents.py
    â”‚   â”œâ”€â”€ test_retrieval.py
    â”‚   â”œâ”€â”€ test_models.py
    â”‚   â””â”€â”€ test_cli.py
    â””â”€â”€ performance/
        â”œâ”€â”€ __init__.py
        â””â”€â”€ test_end_to_end.py      # measure end-to-end search agent timing

```

## ğŸ”„ Workflow Pipeline

The production workflow follows this pipeline:

1. **Planning** - Break down research question into sub-questions
2. **Search** - Multi-provider search (Tavily, Brave, Firecrawl)
3. **Fetch & Parse** - Scrape content, create chunks, generate embeddings
4. **Evidence Selection** - Semantic search + scoring to find best evidence
5. **Synthesis** - LLM generates answer with citations
6. **Critique** - Verify completeness and accuracy
7. **Fact-Check Gate** - Ensure claims have sufficient evidence
8. **Finalize** - Export evidence pack with full traceability

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

- Add more search providers (Google, DuckDuckGo)
- Improve chunking strategies (semantic chunking)
- Add more embedding models
- Enhance citation formatting
- Add web UI
- Add API server mode

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ› Troubleshooting

### "Module not found" errors
```bash
# Make sure you're in the virtual environment
source .venv/bin/activate

# Reinstall in editable mode
pip install -e .
```

### "API key not found"
```bash
# Check your .env file exists
cat .env

# Verify it contains your keys
# OPENAI_API_KEY=sk-...
# FIRECRAWL_API_KEY=fc-...
```

### Tests failing
```bash
# Some tests require actual API keys
# Skip integration tests if needed
pytest -m "not integration"
```

## ğŸ“§ Support

For issues and questions:
- Open an issue on GitHub
- Check existing issues for solutions

---

Built with â¤ï¸ using LangChain and LangGraph
